{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05c22dcb",
   "metadata": {},
   "source": [
    "#Import the necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from efficient_apriori import apriori\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eb7576",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd21fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the data and clean the dataframes\n",
    "\n",
    "# Load data \n",
    "df_sales = pd.read_excel(\"Your path here\")\n",
    "df_Item = pd.read_excel(\"Your path here\")\n",
    "df_Customers = pd.read_excel(\"Your path here\")\n",
    "\n",
    "# Clean dataframes \n",
    "Dataframes = [df_Item, df_Customers, df_sales]\n",
    "for idx, df in enumerate(Dataframes):\n",
    "    columns_before = df.columns.tolist()\n",
    "    df.dropna(axis=1, how=\"all\", inplace=True)\n",
    "\n",
    "    # Track removed columns\n",
    "    cols_after = df.columns.to_list()\n",
    "    removed_cols = set(columns_before) - set(cols_after)\n",
    "    # Print results\n",
    "    print(f\"DataFrame {idx}: Removed columns with all missing values: {removed_cols or 'None'}\")\n",
    "    print(f\"Remaining columns: {cols_after}\\n\")\n",
    "\n",
    "# Merge dataframes\n",
    "df_sales['SoldToCustomer_CD'] = df_sales['SoldToCustomer_CD'].astype(object)\n",
    "merged_d = df_sales.merge(df_Customers, left_on=\"SoldToCustomer_CD\", right_on=\"Customer_CD\", how=\"left\")\n",
    "merged_df = merged_d.merge(df_Item, how=\"left\", on=[\"Item_CD\"])\n",
    "\n",
    "# Columns to keep\n",
    "columns_to_keep = [\n",
    "    'SalesDocumentNumber',\n",
    "    \"BrandL1_TX\",\n",
    "    'Date_CD',\n",
    "    'Item_CD',\n",
    "    'SoldToCustomer_CD',\n",
    "    'NetSalesValue',\n",
    "    'Item_TX',\n",
    "    'CategoryL1_TX',\n",
    "    'ItemStatus_CD',\n",
    "    'Customer_TX',\n",
    "    'ParentCustomer_CD',\n",
    "    'ParentCustomer_TX',\n",
    "    'CustomerGroup_CD',\n",
    "    'CustomerGroup_TX',\n",
    "    'CustomerGroupL1_CD',\n",
    "    'CustomerGroupL1_TX',\n",
    "    \"SalesQuantity\",\n",
    "    \"Customer_CD\"\n",
    "]\n",
    "\n",
    "# Keep specified columns\n",
    "merged_df = merged_df[columns_to_keep]\n",
    "\n",
    "# Remove rows with missing values \n",
    "merged_df.dropna(how='all', inplace=True)\n",
    "\n",
    "# Date_CD column to datetime format\n",
    "merged_df['Date_CD'] = pd.to_datetime(merged_df['Date_CD'], errors='coerce')\n",
    "\n",
    "# Filter the DataFrame \n",
    "merged_df = merged_df[columns_to_keep]\n",
    "\n",
    "# Remove rows where SalesQuantity is negative\n",
    "merged_df = merged_df[merged_df['SalesQuantity'] >= 0]\n",
    "\n",
    "# Calculate Net Sales per Kilo\n",
    "merged_df['Net Sales per Kilo'] = merged_df.apply(\n",
    "    lambda row: row['NetSalesValue'] / row['SalesQuantity']\n",
    "    if pd.notnull(row['NetSalesValue']) and row['SalesQuantity'] > 0 else None,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "##Defining frecuency\n",
    "\n",
    "invoice_freq = merged_df.groupby('CustomerGroupL1_CD')['SalesDocumentNumber'].nunique().reset_index()\n",
    "invoice_freq.columns = ['CustomerGroupL1_CD', 'Invoice_Frequency']\n",
    "\n",
    "merged_df['YearMonth'] = merged_df['Date_CD'].dt.to_period('M')\n",
    "monthly_orders = merged_df.groupby(['CustomerGroupL1_CD', 'YearMonth'])['SalesDocumentNumber'].nunique().reset_index()\n",
    "\n",
    "# Now average monthly frequency\n",
    "monthly_freq = monthly_orders.groupby('CustomerGroupL1_CD')['SalesDocumentNumber'].mean().reset_index()\n",
    "monthly_freq.columns = ['CustomerGroupL1_CD', 'Avg_Monthly_Orders']\n",
    "\n",
    "# Create new invoice key\n",
    "merged_df['InvoiceKey'] = (\n",
    "    merged_df['CustomerGroupL1_CD'].astype(str) + \"_\" + merged_df['Date_CD'].dt.strftime(\"%Y-%m-%d\")\n",
    ")\n",
    "\n",
    "# Preview how many unique transactions this defines\n",
    "print(\"Unique transactions (InvoiceKey):\", merged_df['InvoiceKey'].nunique())\n",
    "\n",
    "raw_txn_count = (\n",
    "    merged_df.groupby([\"SoldToCustomer_CD\", \"Date_CD\"])[\"SalesDocumentNumber\"]\n",
    "    .nunique()\n",
    "    .reset_index(name=\"Raw_Transaction_Count\")\n",
    ")\n",
    "\n",
    "print(f\"Total raw transactions (store+day level): {len(raw_txn_count)}\")\n",
    "\n",
    "\n",
    "merged_df['InvoiceKey'] = (\n",
    "    merged_df['CustomerGroupL1_CD'].astype(str) + \"_\" +\n",
    "    merged_df['Date_CD'].dt.strftime(\"%Y-%m-%d\")\n",
    ")\n",
    "\n",
    "# Counting how many unique InvoiceKeys exist\n",
    "unique_invoicekey_count = merged_df['InvoiceKey'].nunique()\n",
    "\n",
    "print(f\" Total aggregated transactions (group+day level): {unique_invoicekey_count}\")\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Metric': ['Number of Invoices', 'Aggregated invoices by day'],\n",
    "    'Transaction_Count': [len(raw_txn_count), unique_invoicekey_count]\n",
    "})\n",
    "\n",
    "print(comparison)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.bar(comparison['Metric'], comparison['Transaction_Count'], color=['skyblue', 'orange'])\n",
    "plt.title(\"Comparison of Transaction Definitions\")\n",
    "plt.ylabel(\"Number of Transactions\")\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "columns_to_keep = [\n",
    "    'SalesDocumentNumber',\n",
    "    \"BrandL1_TX\",\n",
    "    'Date_CD',\n",
    "    'Item_CD',\n",
    "    'SoldToCustomer_CD',\n",
    "    'NetSalesValue',\n",
    "    'Item_TX',\n",
    "    'CategoryL1_TX',\n",
    "    'ItemStatus_CD',\n",
    "    'Customer_TX',\n",
    "    'ParentCustomer_CD',\n",
    "    'ParentCustomer_TX',\n",
    "    'CustomerGroup_CD',\n",
    "    'CustomerGroup_TX',\n",
    "    'CustomerGroupL1_CD',\n",
    "    'CustomerGroupL1_TX',\n",
    "    \"InvoiceKey\",\n",
    "    \"SalesQuantity\",\n",
    "    \"Customer_CD\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3bdddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Contextual data cleaning (Optional)\n",
    "\n",
    "####Deleting transactions with only one item\n",
    "\n",
    "# Counting items per invoice\n",
    "invoicekey_item_counts = merged_df.groupby(\"InvoiceKey\")['Item_CD'].nunique().reset_index()\n",
    "invoicekey_item_counts.columns = [\"InvoiceKey\", 'ItemCount']\n",
    "\n",
    "# Filtering InvoiceKeys with more than one item\n",
    "valid_invoicekeys = invoicekey_item_counts[invoicekey_item_counts['ItemCount'] > 1]['InvoiceKey']\n",
    "\n",
    "# Filtering merged_df to only keep those valid transactions\n",
    "merged_df = merged_df[merged_df['InvoiceKey'].isin(valid_invoicekeys)].copy()\n",
    "\n",
    "print(f\" Transactions dropped: {merged_df['InvoiceKey'].nunique()} kept from original set.\")\n",
    "\n",
    "\n",
    "original_day_txns = merged_df['InvoiceKey'].nunique()\n",
    "print(f\" Original day-level transactions: {original_day_txns}\")\n",
    "\n",
    "   \n",
    "invoicekey_item_counts = merged_df.groupby('InvoiceKey')['Item_CD'].nunique().reset_index()\n",
    "invoicekey_item_counts.columns = ['InvoiceKey', 'ItemCount']\n",
    "\n",
    "\n",
    "valid_invoicekeys = invoicekey_item_counts[invoicekey_item_counts['ItemCount'] > 1]['InvoiceKey']\n",
    "merged_df_filtered = merged_df[merged_df['InvoiceKey'].isin(valid_invoicekeys)].copy()\n",
    "\n",
    "\n",
    "filtered_day_txns = merged_df_filtered['InvoiceKey'].nunique()\n",
    "print(f\" Kept day-level transactions after dropping 1-item baskets: {filtered_day_txns}\")\n",
    "\n",
    "\n",
    "dropped = original_day_txns - filtered_day_txns\n",
    "print(f\" Dropped day-level transactions with only 1 item: {dropped}\")\n",
    "\n",
    "\n",
    "merged_df = merged_df[columns_to_keep]\n",
    "\n",
    "# Remove rows with missing values in the specified columns\n",
    "merged_df.dropna(how='all', inplace=True)\n",
    "\n",
    "\n",
    "merged_df = merged_df[columns_to_keep]\n",
    "\n",
    "# Remove rows where SalesQuantity is negative\n",
    "merged_df = merged_df[merged_df['SalesQuantity'] >= 0]\n",
    "\n",
    "# Calculate Net Sales per Kilo\n",
    "\n",
    "merged_df['Net Sales per Kilo'] = merged_df.apply(\n",
    "    lambda row: row['NetSalesValue'] / row['SalesQuantity']\n",
    "    if pd.notnull(row['NetSalesValue']) and row['SalesQuantity'] > 0 else None,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "##Defining frecuency\n",
    "\n",
    "invoice_freq = merged_df.groupby('CustomerGroupL1_CD')['SalesDocumentNumber'].nunique().reset_index()\n",
    "invoice_freq.columns = ['CustomerGroupL1_CD', 'Invoice_Frequency']\n",
    "\n",
    "merged_df['YearMonth'] = merged_df['Date_CD'].dt.to_period('M')\n",
    "monthly_orders = merged_df.groupby(['CustomerGroupL1_CD', 'YearMonth'])['SalesDocumentNumber'].nunique().reset_index()\n",
    "\n",
    "\n",
    "monthly_freq = monthly_orders.groupby('CustomerGroupL1_CD')['SalesDocumentNumber'].mean().reset_index()\n",
    "monthly_freq.columns = ['CustomerGroupL1_CD', 'Avg_Monthly_Orders']\n",
    "\n",
    "\n",
    "\n",
    "####Deleting transactions with only one item\n",
    "\n",
    "\n",
    "\n",
    "invoice_item_counts = merged_df.groupby('InvoiceKey')['Item_CD'].nunique().reset_index()\n",
    "invoice_item_counts.columns = ['InvoiceKey', 'ItemCount']\n",
    "\n",
    "\n",
    "one_item_invoices = invoice_item_counts[invoice_item_counts['ItemCount'] == 1]\n",
    "print(f\"Number of invoices with only one item: {len(one_item_invoices)}\")\n",
    "\n",
    "\n",
    "unwanted_brands = ['Category 1', 'Category 2', 'Category 3']\n",
    "merged_df = merged_df[~merged_df['BrandL1_TX'].isin(unwanted_brands)]\n",
    "merged_df = merged_df[merged_df['BrandL1_TX'].notna()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3419af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pickle save and load\n",
    "\n",
    "# Export merged_df as a pickle file\n",
    "merged_df.to_pickle(\"Your path here\")\n",
    "\n",
    "\n",
    "### Load the pickle file\n",
    "merged_df = pd.read_pickle(\"Your path here\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa95c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data analysis and visualization\n",
    "\n",
    "merged_df['Date_CD'] = pd.to_datetime(merged_df['Date_CD'], errors='coerce')\n",
    "\n",
    "\n",
    "merged_df['YearMonth'] = merged_df['Date_CD'].dt.to_period('M')\n",
    "\n",
    "\n",
    "target_brands = ['brand 3', 'brand 2','brand 1']\n",
    "brand_data = merged_df[merged_df['BrandL1_TX'].isin(target_brands)].copy()\n",
    "\n",
    "\n",
    "monthly_sales = brand_data.groupby(['BrandL1_TX', 'YearMonth'])['NetSalesValue'].sum().reset_index()\n",
    "\n",
    "\n",
    "monthly_sales['YearMonth'] = monthly_sales['YearMonth'].dt.to_timestamp()\n",
    "\n",
    "\n",
    "merged_df['Date_CD'] = pd.to_datetime(merged_df['Date_CD'], errors='coerce')\n",
    "merged_df['YearMonth'] = merged_df['Date_CD'].dt.to_period('M')\n",
    "\n",
    "# Filter for 2024\n",
    "df_2024 = merged_df[merged_df['Date_CD'].dt.year == 2024].copy()\n",
    "\n",
    "\n",
    "monthly_2024 = (\n",
    "    df_2024\n",
    "    .groupby(['Item_TX', 'YearMonth','BrandL1_TX'])['NetSalesValue']\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "\n",
    "monthly_2024['YearMonth'] = monthly_2024['YearMonth'].dt.to_timestamp()\n",
    "\n",
    "\n",
    "\n",
    "monthly_2024_jd = monthly_2024[monthly_2024['BrandL1_TX'] == 'Brand name'].copy()\n",
    "\n",
    "\n",
    "pivot_2024 = monthly_2024_jd.pivot_table(\n",
    "    index='YearMonth',\n",
    "    columns='Item_TX',\n",
    "    values='NetSalesValue',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=monthly_sales, x='YearMonth', y='NetSalesValue', hue='BrandL1_TX', marker='o')\n",
    "plt.title(' Monthly Net Sales: brand 1 vs. brand 2 vs. brand 3')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Net Sales Value (â‚¬)')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "pivot_2024 = monthly_2024.pivot_table(\n",
    "    index='YearMonth',\n",
    "    columns='Item_TX',\n",
    "    values='NetSalesValue',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "\n",
    "start_month = pivot_2024.index.min()\n",
    "end_month = pivot_2024.index.max()\n",
    "\n",
    "growth_rates = ((pivot_2024.loc[end_month] - pivot_2024.loc[start_month]) /\n",
    "                pivot_2024.loc[start_month].replace(0, 1)) * 100\n",
    "\n",
    "\n",
    "top_growth_items = growth_rates.sort_values(ascending=False).head(5).index.tolist()\n",
    "\n",
    "print(\" Top Growing Products in 2024:\")\n",
    "for item in top_growth_items:\n",
    "    print(f\"- {item}: {growth_rates[item]:.2f}% growth\")\n",
    "\n",
    "\n",
    "\n",
    "top_growth_sales = monthly_2024[monthly_2024['Item_TX'].isin(top_growth_items)]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.lineplot(data=top_growth_sales, x='YearMonth', y='NetSalesValue', hue='Item_TX', marker='o')\n",
    "plt.title(\"Sales of Top 5 Growing Products (2024)\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Net Sales (â‚¬)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "merged_df['Date_CD'] = pd.to_datetime(merged_df['Date_CD'], errors='coerce')\n",
    "merged_df['YearMonth'] = merged_df['Date_CD'].dt.to_period('M')\n",
    "\n",
    "# Filter for 2024\n",
    "df_2024 = merged_df[merged_df['Date_CD'].dt.year == 2024].copy()\n",
    "\n",
    "# Group monthly sales by item\n",
    "monthly_2024 = (\n",
    "    df_2024\n",
    "    .groupby(['Item_TX', 'YearMonth'])['NetSalesValue']\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "monthly_2024['YearMonth'] = monthly_2024['YearMonth'].dt.to_timestamp()\n",
    "\n",
    "# Pivot: rows = months, columns = items\n",
    "pivot_2024 = monthly_2024.pivot_table(\n",
    "    index='YearMonth',\n",
    "    columns='Item_TX',\n",
    "    values='NetSalesValue',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "# Calculate growth from first to last month\n",
    "start_month = pivot_2024.index.min()\n",
    "end_month = pivot_2024.index.max()\n",
    "\n",
    "growth_rates = ((pivot_2024.loc[end_month] - pivot_2024.loc[start_month]) /\n",
    "                pivot_2024.loc[start_month].replace(0, 1)) * 100\n",
    "\n",
    "# Top 5 growing items\n",
    "top_growth_items = growth_rates.sort_values(ascending=False).head(5).index.tolist()\n",
    "\n",
    "print(\" Top Growing Products in 2024:\")\n",
    "for item in top_growth_items:\n",
    "    print(f\"- {item}: {growth_rates[item]:.2f}% growth\")\n",
    "\n",
    "# Plot top growing products\n",
    "top_growth_sales = monthly_2024[monthly_2024['Item_TX'].isin(top_growth_items)]\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.lineplot(data=top_growth_sales, x='YearMonth', y='NetSalesValue', hue='Item_TX', marker='o')\n",
    "plt.title(\" Sales of Top 5 Growing Products (2024)\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Net Sales (â‚¬)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19b1d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Additional features for clustering and normalizing\n",
    "\n",
    "# 1. Average Days Between Orders per Customer\n",
    "# Sort invoices by customer and date, then calculate time differences\n",
    "merged_df_sorted = merged_df.sort_values(by=['CustomerGroupL1_CD', 'Date_CD'])\n",
    "merged_df_sorted['Days_Since_Last_Order'] = merged_df_sorted.groupby('CustomerGroupL1_CD')['Date_CD'].diff().dt.days\n",
    "\n",
    "# Now calculate average days between orders\n",
    "avg_days_between_orders = merged_df_sorted.groupby('CustomerGroupL1_CD')['Days_Since_Last_Order'].mean().reset_index()\n",
    "avg_days_between_orders.columns = ['CustomerGroupL1_CD', 'Avg_Days_Between_Orders']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# NetSalesValue per customer per brand\n",
    "brand_spend = merged_df.groupby(['CustomerGroupL1_CD', 'BrandL1_TX'])['NetSalesValue'].sum().reset_index()\n",
    "\n",
    "#  NetSalesValue per customer\n",
    "total_spend = merged_df.groupby('CustomerGroupL1_CD')['NetSalesValue'].sum().reset_index()\n",
    "total_spend.columns = ['CustomerGroupL1_CD', 'Total_NetSales']\n",
    "\n",
    "# Spend per brand\n",
    "brand_spend = brand_spend.merge(total_spend, on='CustomerGroupL1_CD')\n",
    "brand_spend['Brand_Spend_Percent'] = brand_spend['NetSalesValue'] / brand_spend['Total_NetSales']\n",
    "\n",
    "brand_spend_matrix = brand_spend.pivot_table(\n",
    "    index='CustomerGroupL1_CD',\n",
    "    columns='BrandL1_TX',\n",
    "    values='Brand_Spend_Percent',\n",
    "    fill_value=0  \n",
    ").reset_index()\n",
    "\n",
    "\n",
    "brand_spend_matrix.columns = ['CustomerGroupL1_CD'] + [f'Brand_Percent_{col}' for col in brand_spend_matrix.columns if col != 'CustomerGroupL1_CD']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  Calculate invoice frequency per customer (number of unique invoice dates)\n",
    "invoice_freq_all = merged_df.groupby('CustomerGroupL1_CD')['Date_CD'].nunique().rename(\"All_Time_Freq\")\n",
    "invoice_freq_2024 = df_2024.groupby('CustomerGroupL1_CD')['Date_CD'].nunique().rename(\"Freq_2024\")\n",
    "\n",
    "\n",
    "freq_df = pd.concat([invoice_freq_all, invoice_freq_2024], axis=1).fillna(0)\n",
    "\n",
    "#  Plot histogram of both frequencies\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(freq_df[\"All_Time_Freq\"], bins=20, color='skyblue', label='All Time', kde=False)\n",
    "sns.histplot(freq_df[\"Freq_2024\"], bins=20, color='salmon', label='2024', kde=False)\n",
    "\n",
    "plt.title(\" Invoice Frequency Distribution: All Time vs. 2024\")\n",
    "plt.xlabel(\"Number of Invoices per Customer\")\n",
    "plt.ylabel(\"Customer Count\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Aggregate metrics per customer\n",
    "customer_summary = merged_df.groupby('CustomerGroupL1_CD').agg(\n",
    "    Total_Kilos=('SalesQuantity', 'sum'),\n",
    "    Total_NetSales=('NetSalesValue', 'sum'),\n",
    "    Invoice_Frequency=('InvoiceKey', pd.Series.nunique)\n",
    ").reset_index()\n",
    "\n",
    "# Calculate Net Sales per Kilo\n",
    "customer_summary['Avg_NetSalesPerKilo'] = customer_summary['Total_NetSales'] / customer_summary['Total_Kilos']\n",
    "\n",
    "customer_summary = customer_summary.merge(avg_days_between_orders, on='CustomerGroupL1_CD', how='left')\n",
    "customer_summary = customer_summary.merge(brand_spend_matrix, on='CustomerGroupL1_CD', how='left')\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "#Capping the clustering features at the 90 percentile\n",
    "features = ['Total_Kilos', 'Total_NetSales', 'Avg_NetSalesPerKilo', 'Invoice_Frequency']\n",
    "\n",
    "customer_summary_capped = customer_summary.copy()\n",
    "\n",
    "\n",
    "for feature in features:\n",
    "    cap_value = customer_summary_capped[feature].quantile(0.90)\n",
    "    customer_summary_capped[feature] = np.where(\n",
    "        customer_summary_capped[feature] > cap_value,\n",
    "        cap_value,\n",
    "        customer_summary_capped[feature]\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.boxplot(data=customer_summary_capped[features])\n",
    "plt.title(\"Boxplot After Winsorization (99th Percentile Cap)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "# Use capped version\n",
    "valid_rows = customer_summary_capped[features].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "valid_data = valid_rows.copy()\n",
    "valid_ids = customer_summary_capped.loc[valid_rows.index, 'CustomerGroupL1_CD'].reset_index(drop=True)\n",
    "\n",
    "\n",
    "normalized = scaler.fit_transform(valid_data) \n",
    "###Prefered a DataFrame for better readability instead of a numpy array\n",
    "\n",
    "normalized_df = pd.DataFrame(\n",
    "    scaler.fit_transform(valid_data),\n",
    "    columns=valid_data.columns\n",
    ")\n",
    "\n",
    "customer_cluster_input = pd.concat([customer_summary_capped[[\"CustomerGroupL1_CD\"]],normalized_df], axis=1)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plot_df = customer_cluster_input.copy()\n",
    "\n",
    "#create pairplot of the normalized data\n",
    "\n",
    "sns.pairplot(plot_df.drop(columns=['CustomerGroupL1_CD']))\n",
    "plt.suptitle(\"Pairwise Plot of Normalized Customer Features\", y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98109c00",
   "metadata": {},
   "source": [
    "Clustering stage: Grouping the different users into differentiated groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44435af",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Defining best K for KMeans\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "k_range = range(2, 11)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans_test = KMeans(n_clusters=k, random_state=42)\n",
    "    labels_k = kmeans_test.fit_predict(normalized_df)\n",
    "    inertias.append(kmeans_test.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(normalized_df, labels_k))\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(k_range, inertias, marker='o')\n",
    "plt.title(\"Elbow Method\")\n",
    "plt.xlabel(\"Number of Clusters (k)\")\n",
    "plt.ylabel(\"Inertia\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(k_range, silhouette_scores, marker='o')\n",
    "plt.title(\"Silhouette Scores\")\n",
    "plt.xlabel(\"Number of Clusters (k)\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Exact scores\n",
    "for k, inertia, sil in zip(k_range, inertias, silhouette_scores):\n",
    "    print(f\"k={k}: Inertia={inertia:.2f}, Silhouette Score={sil:.4f}\")\n",
    "\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "labels = kmeans.fit_predict(normalized_df)\n",
    "\n",
    "clustered_customers = pd.DataFrame({\n",
    "    'CustomerGroupL1_CD': valid_ids,\n",
    "    'Cluster': labels\n",
    "})\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "inertias = []\n",
    "k_range = range(1, 11)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(normalized_df)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "scores = []\n",
    "for k in range(2, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    labels = kmeans.fit_predict(normalized_df)\n",
    "    score = silhouette_score(normalized_df, labels)\n",
    "    scores.append(score)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "inertias = []\n",
    "silhouettes = []\n",
    "k_range = range(2, 11)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    labels = kmeans.fit_predict(normalized_df)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouettes.append(silhouette_score(normalized_df, labels))\n",
    "\n",
    "\n",
    "\n",
    "best_k = 3\n",
    "kmeans_final = KMeans(n_clusters=best_k, random_state=42)\n",
    "final_labels = kmeans_final.fit_predict(normalized_df)\n",
    "\n",
    "# Assign cluster labels back to customers\n",
    "clustered_customers = pd.DataFrame({\n",
    "    'CustomerGroupL1_CD': valid_ids,\n",
    "    'Cluster': final_labels\n",
    "})\n",
    "\n",
    "# Merge clusters into customer_summary\n",
    "customer_summary_clustered_capped = customer_summary_capped.merge(clustered_customers, on='CustomerGroupL1_CD', how='inner')\n",
    "\n",
    "\n",
    "\n",
    "print(customer_summary_clustered_capped['Cluster'].value_counts())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6b6992",
   "metadata": {},
   "source": [
    "Graphic Visualization of clusters and their centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1ce54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "pca_features = normalized_df  # Final version\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(pca_features)\n",
    "\n",
    "# Create a plotting DataFrame\n",
    "pca_df = pd.DataFrame({\n",
    "    'PCA1': pca_result[:, 0],\n",
    "    'PCA2': pca_result[:, 1],\n",
    "    'Cluster': clustered_customers['Cluster'].values  #Final clusters\n",
    "})\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.scatterplot(data=pca_df, x='PCA1', y='PCA2', hue='Cluster', palette='tab10', s=60)\n",
    "plt.title(' PCA: Customer Clusters Visualized in 2D')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plot_df = customer_summary_clustered_capped[['Cluster'] + features].copy()\n",
    "\n",
    "melted = plot_df.melt(id_vars='Cluster', var_name='Feature', value_name='Value')\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(data=melted, x='Feature', y='Value', hue='Cluster')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Feature Distributions by Cluster')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "normalized_plot_df = normalized_df.copy()\n",
    "normalized_plot_df['Cluster'] = clustered_customers['Cluster'].values\n",
    "\n",
    "\n",
    "\n",
    "melted_norm = normalized_plot_df.melt(id_vars='Cluster', var_name='Feature', value_name='Normalized Value')\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(data=melted_norm, x='Feature', y='Normalized Value', hue='Cluster')\n",
    "plt.title(' Normalized Feature Distribution by Cluster')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# How many customers per cluster\n",
    "print(customer_summary_clustered_capped['Cluster'].value_counts())\n",
    "\n",
    "\n",
    "customer_summary_clustered_capped['Cluster'].value_counts().plot(kind='bar')\n",
    "plt.title('ðŸ§® Customers per Cluster')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Number of Customers')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "####Analytics\n",
    "\n",
    "# Select relevant columns\n",
    "metrics = ['Total_Kilos', 'Total_NetSales', 'Avg_NetSalesPerKilo', 'Invoice_Frequency']\n",
    "\n",
    "# Overall summary statistics\n",
    "print(\" General Summary Statistics (All Customers):\")\n",
    "general_stats = customer_summary_clustered_capped[metrics].describe().T.round(2)\n",
    "print(general_stats)\n",
    "\n",
    "# %%\n",
    "# Summary statistics by cluster\n",
    "print(\"\\n Summary Statistics by Cluster:\")\n",
    "cluster_stats = customer_summary_clustered_capped.groupby('Cluster')[metrics].describe().round(2)\n",
    "print(cluster_stats)\n",
    "\n",
    "\n",
    "cluster_stats_flat = cluster_stats.stack().unstack(1).round(2)\n",
    "#cluster_stats_flat.to_excel(\"cluster_metrics_summary.xlsx\")\n",
    "\n",
    "# %%\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Metrics you want to plot\n",
    "metrics = ['Total_Kilos', 'Total_NetSales', 'Avg_NetSalesPerKilo', 'Invoice_Frequency']\n",
    "\n",
    "# Group by cluster\n",
    "cluster_means = customer_summary_clustered_capped.groupby('Cluster')[metrics].mean()\n",
    "cluster_stds = customer_summary_clustered_capped.groupby('Cluster')[metrics].std()\n",
    "\n",
    "# Plot each metric\n",
    "for metric in metrics:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    y = cluster_means[metric]\n",
    "    err = cluster_stds[metric]\n",
    "\n",
    "    # Plot with error bars (std dev)\n",
    "    plt.bar(y.index.astype(str), y.values, yerr=err.values, capsize=5, color='skyblue')\n",
    "    plt.title(f\" {metric} by Cluster\")\n",
    "    plt.xlabel(\"Cluster\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.grid(True, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# List of variables to plot\n",
    "metrics = ['Total_Kilos', 'Total_NetSales', 'Avg_NetSalesPerKilo', 'Invoice_Frequency']\n",
    "\n",
    "# One boxplot per metric\n",
    "for metric in metrics:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.boxplot(data=customer_summary_clustered_capped, x='Cluster', y=metric, palette='Set2')\n",
    "    plt.title(f\" Distribution of {metric} by Cluster\")\n",
    "    plt.xlabel(\"Cluster\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.grid(True, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "brand_cols = [col for col in customer_summary_clustered_capped.columns if col.startswith(\"Brand_Percent_\")]\n",
    "\n",
    "melted_brand_spend = customer_summary_clustered_capped.melt(\n",
    "    id_vars='Cluster',\n",
    "    value_vars=brand_cols,\n",
    "    var_name='Brand',\n",
    "    value_name='Percent_Spent'\n",
    ")\n",
    "\n",
    "# Average spend per brand per cluster\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.barplot(data=melted_brand_spend, x='Brand', y='Percent_Spent', hue='Cluster')\n",
    "plt.title(\" Average Brand Spend % by Cluster\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel(\"Average % of Spend\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# PCA for feature contributions\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Run PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(normalized_df)\n",
    "\n",
    "# Create a DataFrame with feature loadings\n",
    "loadings = pd.DataFrame(\n",
    "    pca.components_.T,\n",
    "    columns=['PC1', 'PC2'],\n",
    "    index=normalized_df.columns\n",
    ")\n",
    "\n",
    "# Plot feature contributions\n",
    "ax = loadings.plot(kind='barh', figsize=(10, 6), title=' Feature Contributions to Principal Components')\n",
    "plt.axvline(0, color='black', linestyle='--', linewidth=1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display top contributing features for each PC\n",
    "print(\"\\n Top Contributors to PC1:\")\n",
    "print(loadings['PC1'].abs().sort_values(ascending=False).head(5))\n",
    "\n",
    "print(\"\\n Top Contributors to PC2:\")\n",
    "print(loadings['PC2'].abs().sort_values(ascending=False).head(5))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b890450b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transactions labeled with clusters:\n",
    "merged_df_clustered = merged_df.merge(\n",
    "    customer_summary_clustered_capped[['CustomerGroupL1_CD','Cluster']],\n",
    "    on='CustomerGroupL1_CD',\n",
    "    how='inner'\n",
    ")\n",
    "merged_df_clustered['Cluster'] = merged_df_clustered['Cluster'].astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f30c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Defining the most relevant associations between propducts \n",
    "\n",
    "print(\"DataFrame shape:\", clustered_df.shape)\n",
    "print(\"Columns:\", clustered_df.columns.tolist())\n",
    "\n",
    "\n",
    "print(clustered_df.head(10).to_string(index=False))\n",
    "\n",
    "\n",
    "for cid in sorted(clustered_df['cluster'].unique()):\n",
    "    print(f\"\\n--- Cluster {cid} ---\")\n",
    "    sub = clustered_df[clustered_df['cluster']==cid][\n",
    "        ['antecedent','consequent','cust_bought_ant','cust_diff_count']\n",
    "    ]\n",
    "# rule + counts\n",
    "    print(sub.to_string(index=False))\n",
    "\n",
    "summary = clustered_df[['cluster','antecedent','consequent','cust_bought_ant','cust_diff_count']]\n",
    "print(\"\\nAll clusters, all rules:\\n\", summary.to_string(index=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e89d5cf",
   "metadata": {},
   "source": [
    "Use the Association rule Library Apriori - Rule mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f11a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# All transactions\n",
    "df_all = merged_df_clustered.copy()\n",
    "#BY INVOICE\n",
    "transactions = df_all.groupby('InvoiceKey')['Item_TX'].apply(list).tolist()\n",
    "\n",
    "#Apriori over entire dataset\n",
    "itemsets, rules = apriori(\n",
    "    transactions,\n",
    "    min_support=0.05,\n",
    "    min_confidence=0.5\n",
    ")\n",
    "\n",
    "def extract_counts(rule, df):\n",
    "    ant, cons = set(rule.lhs), set(rule.rhs)\n",
    "    cust_ant  = set(df[df['Item_TX'].isin(ant)]['ParentCustomer_CD'])\n",
    "    cust_cons = set(df[df['Item_TX'].isin(cons)]['ParentCustomer_CD'])\n",
    "    diff      = cust_ant - cust_cons\n",
    "    return cust_ant, cust_cons, diff\n",
    "\n",
    "# Prepare three sections: all, top15, next15\n",
    "sorted_rules = sorted(rules, key=lambda r: r.support, reverse=True)\n",
    "sections = {\n",
    "    'All_by_Support' : sorted_rules,\n",
    "    'Top15'          : sorted_rules[:15],\n",
    "    'Next15'         : sorted_rules[15:30],\n",
    "}\n",
    "\n",
    "# Build DataFrames\n",
    "frames = {}\n",
    "for name, sel in sections.items():\n",
    "    recs = []\n",
    "    for rule in sel:\n",
    "        cust_ant, cust_cons, diff = extract_counts(rule, df_all)\n",
    "        recs.append({\n",
    "            'antecedent'      : ', '.join(rule.lhs),\n",
    "            'consequent'      : ', '.join(rule.rhs),\n",
    "            'support'         : rule.support,\n",
    "            'confidence'      : rule.confidence,\n",
    "            'lift'            : rule.lift,\n",
    "            'cust_bought_ant' : len(cust_ant),\n",
    "            'cust_diff_count' : len(diff),\n",
    "            'cust_diff_list'  : list(diff)\n",
    "        })\n",
    "    frames[name] = pd.DataFrame(recs)\n",
    "\n",
    "# IdÂ´s -Names\n",
    "cust_map = (\n",
    "    df_all[['ParentCustomer_CD','Customer_TX']]\n",
    "    .drop_duplicates()\n",
    "    .set_index('ParentCustomer_CD')['Customer_TX']\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "for df in frames.values():\n",
    "    # clean IDs\n",
    "    df['Missing_ParentCustomer_CDs'] = df['cust_diff_list'].apply(\n",
    "        lambda lst: [int(x) for x in lst if pd.notnull(x)]\n",
    "    )\n",
    "    # map to names\n",
    "    df['Missing_ParentCustomer_TXs'] = df['Missing_ParentCustomer_CDs'].apply(\n",
    "        lambda ids: \"; \".join(cust_map.get(cid, f\"ID:{cid}\") for cid in ids)\n",
    "    )\n",
    "\n",
    "# Export \n",
    "out_path = \"G:/Mi unidad/Antwerp/Data engineering/Solved/Market basket/mba_general_rules_by_support.xlsx\"\n",
    "with pd.ExcelWriter(out_path, engine='openpyxl') as writer:\n",
    "    frames['All_by_Support'].to_excel(writer, sheet_name='General_All_by_Support', index=False)\n",
    "    frames['Top15'].to_excel(writer,      sheet_name='General_Top15',          index=False)\n",
    "    frames['Next15'].to_excel(writer,     sheet_name='General_Next15',         index=False)\n",
    "print(\" General MBA rules exported to:\", out_path)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8,6), tight_layout=True)\n",
    "sns.barplot(\n",
    "    data=frames['Top15'],\n",
    "    y='antecedent',\n",
    "    x='cust_diff_count',\n",
    "    hue='consequent',\n",
    "    dodge=False\n",
    ")\n",
    "plt.title(\" Overall: # Customers Bought Antecedent but NOT Consequent\\n(Top 15 Rules by Support)\")\n",
    "plt.xlabel(\"Number of Customers Missing Consequent\")\n",
    "plt.ylabel(\"Antecedent\")\n",
    "plt.legend(title=\"Consequent\", bbox_to_anchor=(1.05,1), loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "##Counting Nan Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7164ae35",
   "metadata": {},
   "outputs": [],
   "source": [
    "############  Clustered Anlysis\n",
    "\n",
    "#import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from efficient_apriori import apriori\n",
    "\n",
    "\n",
    "###Cleared transactions general\n",
    "\n",
    "# General transactions and run Apriori \n",
    "transactions = merged_df_clustered.groupby('InvoiceKey')['Item_TX'].apply(list).tolist()\n",
    "itemsets, rules = apriori(transactions, min_support=0.05, min_confidence=0.0)\n",
    "\n",
    "\n",
    "def clean_ids(id_list):\n",
    "    return [int(x) for x in id_list if pd.notnull(x)]\n",
    "\n",
    "#  Top-15 by support \n",
    "general_records = []\n",
    "for rule in sorted(rules, key=lambda r: r.support, reverse=True)[:15]:\n",
    "    ant, cons = set(rule.lhs), set(rule.rhs)\n",
    "    cust_ant  = set(merged_df_clustered[merged_df_clustered['Item_TX'].isin(ant)]['ParentCustomer_CD'])\n",
    "    cust_cons = set(merged_df_clustered[merged_df_clustered['Item_TX'].isin(cons)]['ParentCustomer_CD'])\n",
    "    diff = list(cust_ant - cust_cons)\n",
    "\n",
    "    general_records.append({\n",
    "        'antecedent'      : ', '.join(rule.lhs),\n",
    "        'consequent'      : ', '.join(rule.rhs),\n",
    "        'support'         : rule.support,\n",
    "        'confidence'      : rule.confidence,\n",
    "        'lift'            : rule.lift,\n",
    "        'cust_bought_ant' : len(cust_ant),\n",
    "        'raw_diff_list'   : diff,\n",
    "        'clean_diff_list' : clean_ids(diff),\n",
    "        'diff_count'      : len(clean_ids(diff))\n",
    "    })\n",
    "\n",
    "general_top15 = pd.DataFrame(general_records)\n",
    "\n",
    "#  Next-15 by support \n",
    "general_records_2 = []\n",
    "for rule in sorted(rules, key=lambda r: r.support, reverse=True)[15:30]:\n",
    "    ant, cons = set(rule.lhs), set(rule.rhs)\n",
    "    cust_ant  = set(merged_df_clustered[merged_df_clustered['Item_TX'].isin(ant)]['ParentCustomer_CD'])\n",
    "    cust_cons = set(merged_df_clustered[merged_df_clustered['Item_TX'].isin(cons)]['ParentCustomer_CD'])\n",
    "    diff = list(cust_ant - cust_cons)\n",
    "\n",
    "    general_records_2.append({\n",
    "        'antecedent'      : ', '.join(rule.lhs),\n",
    "        'consequent'      : ', '.join(rule.rhs),\n",
    "        'support'         : rule.support,\n",
    "        'confidence'      : rule.confidence,\n",
    "        'lift'            : rule.lift,\n",
    "        'cust_bought_ant' : len(cust_ant),\n",
    "        'raw_diff_list'   : diff,\n",
    "        'clean_diff_list' : clean_ids(diff),\n",
    "        'diff_count'      : len(clean_ids(diff))\n",
    "    })\n",
    "\n",
    "general_next15 = pd.DataFrame(general_records_2)\n",
    "\n",
    "# Build ParentCustomer_CD - ParentCustomer_TX \n",
    "cust_map = (\n",
    "    merged_df_clustered[['ParentCustomer_CD','ParentCustomer_TX']]\n",
    "    .drop_duplicates()\n",
    "    .set_index('ParentCustomer_CD')['ParentCustomer_TX']\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# IDs  -names \n",
    "for df in (general_top15, general_next15):\n",
    "    df['Missing_ParentCustomer_TXs'] = df['clean_diff_list'].apply(\n",
    "        lambda ids: [cust_map.get(i, f\"ID:{i}\") for i in ids]\n",
    "    )\n",
    "    df['Missing_ParentCustomer_TXs_str'] = df['Missing_ParentCustomer_TXs'].apply(lambda L: \"; \".join(L))\n",
    "\n",
    "# Export\n",
    "out_path = \"Your path here\"\n",
    "with pd.ExcelWriter(out_path, engine='openpyxl') as writer:\n",
    "    general_top15 .to_excel(writer, sheet_name='General_Top15', index=False)\n",
    "    general_next15.to_excel(writer, sheet_name='General_Next15', index=False)\n",
    "print(\"Exported with customer-names to:\", out_path)\n",
    "\n",
    "for df, title in [(general_top15, \"Top 15\"), (general_next15, \"Next 15\")]:\n",
    "    pdf = df[df['diff_count'] > 0]\n",
    "    plt.figure(figsize=(8, max(4, len(pdf)*0.5)), tight_layout=True)\n",
    "    sns.barplot(\n",
    "        data=pdf,\n",
    "        y='antecedent',\n",
    "        x='diff_count',\n",
    "        hue='consequent',\n",
    "        dodge=False\n",
    "    )\n",
    "    plt.title(f\" Overall: # Customers Bought Antecedent but NOT Consequent\\n({title} Rules by Support)\")\n",
    "    plt.xlabel(\"Number of Customers Missing Consequent\")\n",
    "    plt.ylabel(\"Antecedent Item\")\n",
    "    plt.legend(title=\"Consequent\", bbox_to_anchor=(1.05,1), loc='upper left')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdb96cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def export_item_support_table(merged_df_clustered, output_path):\n",
    "\n",
    "\n",
    "    merged_df_clustered['Cluster'] = merged_df_clustered['Cluster'].astype(int)\n",
    "\n",
    "    # Total number of unique invoices\n",
    "    total_txns = merged_df_clustered['InvoiceKey'].nunique()\n",
    "    total_by_cluster = merged_df_clustered.groupby('Cluster')['InvoiceKey'].nunique()\n",
    "\n",
    "    # Item support per cluster\n",
    "    item_cluster_support = (\n",
    "        merged_df_clustered\n",
    "        .groupby(['Item_TX', 'Cluster'])['InvoiceKey']\n",
    "        .nunique()\n",
    "        .unstack(fill_value=0)\n",
    "    )\n",
    "\n",
    "    # Normalize to get support\n",
    "    for cluster in item_cluster_support.columns:\n",
    "        item_cluster_support[cluster] = item_cluster_support[cluster] / total_by_cluster[cluster]\n",
    "\n",
    "    # Add overall support\n",
    "    overall_support = (\n",
    "        merged_df_clustered\n",
    "        .groupby('Item_TX')['InvoiceKey']\n",
    "        .nunique()\n",
    "        .div(total_txns)\n",
    "    )\n",
    "    item_cluster_support['Overall'] = overall_support\n",
    "\n",
    "    # Convert to percentage\n",
    "    item_cluster_support_pct = (item_cluster_support * 100).round(1).astype(str) + '%'\n",
    "\n",
    "    # Sort by Overall\n",
    "    item_cluster_support_pct = item_cluster_support_pct.sort_values(by='Overall', ascending=False)\n",
    "\n",
    "    # Export\n",
    "    item_cluster_support_pct.to_excel(output_path)\n",
    "    print(f\" Support report exported to: {output_path}\")\n",
    "\n",
    "# ==== Usage ====\n",
    "output_path = \"Your path here\"\n",
    "export_item_support_table(merged_df_clustered, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956009b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "718e9aca",
   "metadata": {},
   "source": [
    "General Association rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cb6ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clusterâ€specific Apriori to get rule lists\n",
    "from efficient_apriori import apriori\n",
    "\n",
    "cluster_rules_dict = {}\n",
    "for cid in sorted(merged_df_clustered['Cluster'].unique()):\n",
    "    dfc = merged_df_clustered[merged_df_clustered['Cluster']==cid]\n",
    "    txns = dfc.groupby('InvoiceKey')['Item_TX'].apply(list).tolist()\n",
    "    if txns:\n",
    "        itemsets, rules_list = apriori(txns, min_support=0.06, min_confidence=0.5)\n",
    "    else:\n",
    "        rules_list = []\n",
    "    cluster_rules_dict[cid] = rules_list\n",
    "\n",
    "# Top-15â€byâ€support DataFrame per cluster\n",
    "cluster_rows = []\n",
    "for cid, rules_list in cluster_rules_dict.items():\n",
    "    # take the 15 highestâ€support rules\n",
    "    for rule in sorted(rules_list, key=lambda r: r.support, reverse=True)[:15]:\n",
    "        ant, cons = set(rule.lhs), set(rule.rhs)\n",
    "        dfc = merged_df_clustered[merged_df_clustered['Cluster']==cid]\n",
    "        cust_ant  = set(dfc[dfc['Item_TX'].isin(ant)]['ParentCustomer_CD'])\n",
    "        cust_cons = set(dfc[dfc['Item_TX'].isin(cons)]['ParentCustomer_CD'])\n",
    "        diff = cust_ant - cust_cons\n",
    "        \n",
    "        cluster_rows.append({\n",
    "            'cluster':          cid,\n",
    "            'antecedent':       ', '.join(rule.lhs),\n",
    "            'consequent':       ', '.join(rule.rhs),\n",
    "            'support':          rule.support,\n",
    "            'confidence':       rule.confidence,\n",
    "            'lift':             rule.lift,\n",
    "            'cust_bought_ant':  len(cust_ant),\n",
    "            'cust_diff_count':  len(diff),\n",
    "            'cust_diff_list':   list(diff)\n",
    "        })\n",
    "\n",
    "clustered_df = pd.DataFrame(cluster_rows)\n",
    "\n",
    "# 4) Export  the clustered sheet \n",
    "path = \"Your path here\"\n",
    "with pd.ExcelWriter(path, engine='openpyxl') as writer:\n",
    "    clustered_df.to_excel(writer, sheet_name='ByCluster_Top15_by_Support', index=False)\n",
    "\n",
    "print(\" Clustered top-15 rules exported to:\", path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c766d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"DataFrame shape:\", clustered_df.shape)\n",
    "print(\"Columns:\", clustered_df.columns.tolist())\n",
    "\n",
    "\n",
    "print(clustered_df.head(10).to_string(index=False))\n",
    "\n",
    "\n",
    "for cid in sorted(clustered_df['cluster'].unique()):\n",
    "    print(f\"\\n--- Cluster {cid} ---\")\n",
    "    sub = clustered_df[clustered_df['cluster']==cid][\n",
    "        ['antecedent','consequent','cust_bought_ant','cust_diff_count']\n",
    "    ]\n",
    "# rule + counts\n",
    "    print(sub.to_string(index=False))\n",
    "\n",
    "summary = clustered_df[['cluster','antecedent','consequent','cust_bought_ant','cust_diff_count']]\n",
    "print(\"\\nAll clusters, all rules:\\n\", summary.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7475c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Clustered Cleaned count ()\n",
    "\n",
    "\n",
    "\n",
    "def count_valid(ids):\n",
    "\n",
    "    return sum(1 for x in ids if pd.notnull(x))\n",
    "\n",
    "clustered_df['cust_diff_count_clean'] = clustered_df['cust_diff_list'].apply(count_valid)\n",
    "\n",
    "# Drop any rules where that cleaned count is zero\n",
    "plot_df = clustered_df[clustered_df['cust_diff_count_clean'] > 0].copy()\n",
    "\n",
    "\n",
    "for cid in sorted(plot_df['cluster'].unique()):\n",
    "    dfc = plot_df[plot_df['cluster'] == cid]\n",
    "\n",
    "    plt.figure(figsize=(8, max(4, len(dfc)*0.5)))\n",
    "    sns.barplot(\n",
    "        data=dfc,\n",
    "        y='antecedent',\n",
    "        x='cust_diff_count_clean',       # use the cleaned count\n",
    "        hue='consequent',\n",
    "        dodge=False\n",
    "    )\n",
    "    plt.title(f\"Cluster {cid}: # Customers Bought Antecedent but NOT Consequent\")\n",
    "    plt.xlabel(\"Number of Customers Missing Consequent\")\n",
    "    plt.ylabel(\"Antecedent Item\")\n",
    "    plt.legend(title=\"Consequent\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac8ea43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding business opportunities with the customers who bought only one of the two products relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13e7250",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#  list of rules for each cluster.\n",
    "cluster_rules_dict = {}\n",
    "for cid in sorted(merged_df_clustered['Cluster'].unique()):\n",
    "    dfc = merged_df_clustered[merged_df_clustered['Cluster'] == cid]\n",
    "    transactions = dfc.groupby('InvoiceKey')['Item_TX'].apply(list).tolist()\n",
    "    if transactions:\n",
    "        _, rules_list = apriori(transactions, min_support=0.06, min_confidence=0.5)\n",
    "    else:\n",
    "        rules_list = []\n",
    "    cluster_rules_dict[cid] = rules_list\n",
    "\n",
    "# Rules ranked 16â€“30 by support, per cluster \n",
    "cluster_rows = []\n",
    "for cid, rules_list in cluster_rules_dict.items():\n",
    "    # sort by support descending\n",
    "    sorted_rules = sorted(rules_list, key=lambda r: r.support, reverse=True)\n",
    "    \n",
    "    # take rules #16 through #30\n",
    "    for rule in sorted_rules[15:30]:\n",
    "        ant, cons = set(rule.lhs), set(rule.rhs)\n",
    "        dfc = merged_df_clustered[merged_df_clustered['Cluster'] == cid]\n",
    "        \n",
    "        cust_ant  = set(dfc[dfc['Item_TX'].isin(ant)]['ParentCustomer_CD'])\n",
    "        cust_cons = set(dfc[dfc['Item_TX'].isin(cons)]['ParentCustomer_CD'])\n",
    "        diff      = cust_ant - cust_cons\n",
    "        \n",
    "        cluster_rows.append({\n",
    "            'cluster':           cid,\n",
    "            'antecedent':        ', '.join(rule.lhs),\n",
    "            'consequent':        ', '.join(rule.rhs),\n",
    "            'support':           rule.support,\n",
    "            'confidence':        rule.confidence,\n",
    "            'lift':              rule.lift,\n",
    "            'cust_bought_ant':   len(cust_ant),\n",
    "            'cust_diff_list':    list(diff)\n",
    "        })\n",
    "\n",
    "clustered_next15 = pd.DataFrame(cluster_rows)\n",
    "\n",
    "# Clean out NaNs \n",
    "def count_valid(ids):\n",
    "    return sum(1 for x in ids if pd.notnull(x))\n",
    "\n",
    "clustered_next15['cust_diff_count'] = (\n",
    "    clustered_next15['cust_diff_list']\n",
    "    .apply(count_valid)\n",
    ")\n",
    "\n",
    "plot_df = clustered_next15[clustered_next15['cust_diff_count'] > 0].copy()\n",
    "\n",
    "# ParentCustomer_CD â†’ Customer_TX\n",
    "cust_map = (\n",
    "    merged_df_clustered[['ParentCustomer_CD', 'Customer_TX']]\n",
    "    .drop_duplicates()\n",
    "    .set_index('ParentCustomer_CD')['Customer_TX']\n",
    "    .to_dict()\n",
    ")\n",
    "def map_names(id_list):\n",
    "    # filter out any nan\n",
    "    clean_ids = [cid for cid in id_list if pd.notnull(cid)]\n",
    "    # map to names, falling back to â€œID:123â€ if missing\n",
    "    names = [cust_map.get(cid, f\"ID:{int(cid)}\") for cid in clean_ids]\n",
    "    return \"; \".join(names)\n",
    "\n",
    "clustered_df['cust_diff_names'] = clustered_df['cust_diff_list'].apply(map_names)\n",
    "\n",
    "# final export columns\n",
    "export_cols = [\n",
    "    'cluster',\n",
    "    'antecedent',\n",
    "    'consequent',\n",
    "    'support',\n",
    "    'confidence',\n",
    "    'lift',\n",
    "    'cust_bought_ant',\n",
    "    'cust_diff_count',\n",
    "    'cust_diff_list',\n",
    "    'cust_diff_names'\n",
    "]\n",
    "export_df = clustered_df[export_cols].copy()\n",
    "\n",
    "path = \"G:/Mi unidad/Antwerp/Data engineering/Solved/Market basket/\" \\\n",
    "       \"mba_clustered_top15_by_support_with_names.xlsx\"\n",
    "with pd.ExcelWriter(path, engine='openpyxl') as writer:\n",
    "    export_df.to_excel(\n",
    "        writer,\n",
    "        sheet_name='ByCluster_Top15_by_Support',\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "print(\" Clustered top-15 rules (with cust_diff_names) exported to:\", path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8155b99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Dataframe\n",
    "cust_map = (\n",
    "    merged_df_clustered[['ParentCustomer_CD', 'Customer_TX']]\n",
    "    .drop_duplicates()\n",
    "    .set_index('ParentCustomer_CD')['Customer_TX']\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "def map_names(id_list):\n",
    "    names = [cust_map.get(i, f\"ID:{i}\") for i in id_list if pd.notnull(i)]\n",
    "    return \"; \".join(names)\n",
    "\n",
    "clustered_next15['cust_diff_names'] = (\n",
    "    clustered_next15['cust_diff_list']\n",
    "    .apply(map_names)\n",
    ")\n",
    "\n",
    "# Select columns \n",
    "export_cols = [\n",
    "    'cluster',\n",
    "    'antecedent',\n",
    "    'consequent',\n",
    "    'support',\n",
    "    'confidence',\n",
    "    'lift',\n",
    "    'cust_bought_ant',\n",
    "    'cust_diff_count',\n",
    "    'cust_diff_list',\n",
    "    'cust_diff_names'\n",
    "]\n",
    "export_df = clustered_next15[export_cols].copy()\n",
    "\n",
    "# Output\n",
    "output_path = \"Your path here\"\n",
    "\n",
    "with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
    "    export_df.to_excel(\n",
    "        writer,\n",
    "        sheet_name='ByCluster_Next15_by_Support',\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "print(f\" Next-15-by-support clustered rules exported to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac2b50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Cleaned Clustered rules\n",
    "### TOP 15 - TOP 30\n",
    "\n",
    "#Export paths\n",
    "path_top15   = \"Your path here\"\n",
    "path_next15  = \"Your path here\"\n",
    "\n",
    "\n",
    "df1 = (\n",
    "    pd.read_excel(path_top15, sheet_name=\"ByCluster_Top15_by_Support\")\n",
    "      .assign(rule_set=\"1-15\")\n",
    ")\n",
    "df2 = (\n",
    "    pd.read_excel(path_next15, sheet_name=\"ByCluster_Next15_by_Support\")\n",
    "      .assign(rule_set=\"16-30\")\n",
    ")\n",
    "\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Clean NaN values\n",
    "df = df[df['cust_diff_count'].notna()]\n",
    "\n",
    "#Plotting\n",
    "for cid in sorted(df['cluster'].unique()):\n",
    "    for rs in [\"1-15\",\"16-30\"]:\n",
    "        sub = df[(df['cluster']==cid) & (df['rule_set']==rs)]\n",
    "        if sub.empty: \n",
    "            continue\n",
    "\n",
    "        plt.figure(figsize=(10, max(4, len(sub)*0.5)))\n",
    "        ax = sns.barplot(\n",
    "            data=sub,\n",
    "            y=\"antecedent\",\n",
    "            x=\"cust_diff_count\",\n",
    "            hue=\"consequent\",\n",
    "            dodge=False\n",
    "        )\n",
    "        ax.set_title(f\"Cluster {cid} â€“ Rules {rs} â†’ #Cust bought Ant not Cons\")\n",
    "        ax.set_xlabel(\"Count of Customers\")\n",
    "        ax.set_ylabel(\"Antecedent Item\")\n",
    "        ax.legend(title=\"Consequent\", bbox_to_anchor=(1.05,1), loc=\"upper left\")\n",
    "\n",
    "      \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1f6724",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Paths \n",
    "path_top15  = \"Your exporting path\"\n",
    "path_next15 = \"Your exporting path\"\n",
    "\n",
    "df1 = (\n",
    "    pd.read_excel(path_top15, sheet_name=\"ByCluster_Top15_by_Support\")\n",
    "      .assign(rule_set=\"1-15\")\n",
    ")\n",
    "df2 = (\n",
    "    pd.read_excel(path_next15, sheet_name=\"ByCluster_Next15_by_Support\")\n",
    "      .assign(rule_set=\"16-30\")\n",
    ")\n",
    "\n",
    "combined = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "#  Clean \n",
    "combined['cust_diff_count'] = combined['cust_diff_list'].apply(lambda lst: sum(1 for x in lst if pd.notnull(x)))\n",
    "combined = combined[combined['cust_diff_count'] > 0].copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  Build DataFrame, preserving both IDs and names:\n",
    "out = combined[[\n",
    "    'cluster',\n",
    "    'rule_set',\n",
    "    'antecedent',\n",
    "    'consequent',\n",
    "    'support',\n",
    "    'confidence',\n",
    "    'lift',\n",
    "    'cust_bought_ant',\n",
    "    'cust_diff_count',\n",
    "    'cust_diff_list',    # raw ParentCustomer_CD list\n",
    "    'cust_diff_names'    # ParentCustomer_TX list\n",
    "]].rename(columns={\n",
    "    'cluster'         : 'Cluster',\n",
    "    'rule_set'        : 'Rule_Set',\n",
    "    'antecedent'      : 'Antecedent',\n",
    "    'consequent'      : 'Consequent',\n",
    "    'support'         : 'Support',\n",
    "    'confidence'      : 'Confidence',\n",
    "    'lift'            : 'Lift',\n",
    "    'cust_bought_ant' : '#Cust_Bought_Ant',\n",
    "    'cust_diff_count' : '#Cust_Missing_Cons',\n",
    "    'cust_diff_list'  : 'Missing_ParentCustomer_CDs',\n",
    "    'cust_diff_names' : 'Missing_ParentCustomer_TXs'\n",
    "})\n",
    "\n",
    "# 6) Export: one sheet per cluster \n",
    "output_path = \"Your path here\"\n",
    "with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
    "    for cid, sub in out.groupby('Cluster'):\n",
    "        sheet = f\"Cluster_{cid}\"\n",
    "        # drop the Cluster column since sheet name encodes it\n",
    "        sub.drop(columns=['Cluster']).to_excel(writer, sheet_name=sheet, index=False)\n",
    "\n",
    "print(\" Opportunity workbook (with IDs) written to:\", output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a731da",
   "metadata": {},
   "source": [
    "In original code the clustered MBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41120dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# All transactions\n",
    "df_all = merged_df_clustered.copy()\n",
    "#BY INVOICE\n",
    "transactions = df_all.groupby('InvoiceKey')['Item_TX'].apply(list).tolist()\n",
    "\n",
    "#Apriori over entire dataset\n",
    "itemsets, rules = apriori(\n",
    "    transactions,\n",
    "    min_support=0.05,\n",
    "    min_confidence=0.5\n",
    ")\n",
    "\n",
    "def extract_counts(rule, df):\n",
    "    ant, cons = set(rule.lhs), set(rule.rhs)\n",
    "    cust_ant  = set(df[df['Item_TX'].isin(ant)]['ParentCustomer_CD'])\n",
    "    cust_cons = set(df[df['Item_TX'].isin(cons)]['ParentCustomer_CD'])\n",
    "    diff      = cust_ant - cust_cons\n",
    "    return cust_ant, cust_cons, diff\n",
    "\n",
    "# Prepare three sections: all, top15, next15\n",
    "sorted_rules = sorted(rules, key=lambda r: r.support, reverse=True)\n",
    "sections = {\n",
    "    'All_by_Support' : sorted_rules,\n",
    "    'Top15'          : sorted_rules[:15],\n",
    "    'Next15'         : sorted_rules[15:30],\n",
    "}\n",
    "\n",
    "# Build DataFrames\n",
    "frames = {}\n",
    "for name, sel in sections.items():\n",
    "    recs = []\n",
    "    for rule in sel:\n",
    "        cust_ant, cust_cons, diff = extract_counts(rule, df_all)\n",
    "        recs.append({\n",
    "            'antecedent'      : ', '.join(rule.lhs),\n",
    "            'consequent'      : ', '.join(rule.rhs),\n",
    "            'support'         : rule.support,\n",
    "            'confidence'      : rule.confidence,\n",
    "            'lift'            : rule.lift,\n",
    "            'cust_bought_ant' : len(cust_ant),\n",
    "            'cust_diff_count' : len(diff),\n",
    "            'cust_diff_list'  : list(diff)\n",
    "        })\n",
    "    frames[name] = pd.DataFrame(recs)\n",
    "\n",
    "# IdÂ´s -Names\n",
    "cust_map = (\n",
    "    df_all[['ParentCustomer_CD','Customer_TX']]\n",
    "    .drop_duplicates()\n",
    "    .set_index('ParentCustomer_CD')['Customer_TX']\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "for df in frames.values():\n",
    "    # clean IDs\n",
    "    df['Missing_ParentCustomer_CDs'] = df['cust_diff_list'].apply(\n",
    "        lambda lst: [int(x) for x in lst if pd.notnull(x)]\n",
    "    )\n",
    "    # map to names\n",
    "    df['Missing_ParentCustomer_TXs'] = df['Missing_ParentCustomer_CDs'].apply(\n",
    "        lambda ids: \"; \".join(cust_map.get(cid, f\"ID:{cid}\") for cid in ids)\n",
    "    )\n",
    "\n",
    "# Export \n",
    "out_path = \"Your exporting path\"\n",
    "with pd.ExcelWriter(out_path, engine='openpyxl') as writer:\n",
    "    frames['All_by_Support'].to_excel(writer, sheet_name='General_All_by_Support', index=False)\n",
    "    frames['Top15'].to_excel(writer,      sheet_name='General_Top15',          index=False)\n",
    "    frames['Next15'].to_excel(writer,     sheet_name='General_Next15',         index=False)\n",
    "print(\" General MBA rules exported to:\", out_path)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8,6), tight_layout=True)\n",
    "sns.barplot(\n",
    "    data=frames['Top15'],\n",
    "    y='antecedent',\n",
    "    x='cust_diff_count',\n",
    "    hue='consequent',\n",
    "    dodge=False\n",
    ")\n",
    "plt.title(\" Overall: # Customers Bought Antecedent but NOT Consequent\\n(Top 15 Rules by Support)\")\n",
    "plt.xlabel(\"Number of Customers Missing Consequent\")\n",
    "plt.ylabel(\"Antecedent\")\n",
    "plt.legend(title=\"Consequent\", bbox_to_anchor=(1.05,1), loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "##Counting Nan Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f3a33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from efficient_apriori import apriori\n",
    "\n",
    "\n",
    "###Cleared transactions general\n",
    "\n",
    "# General transactions and run Apriori \n",
    "transactions = merged_df_clustered.groupby('InvoiceKey')['Item_TX'].apply(list).tolist()\n",
    "itemsets, rules = apriori(transactions, min_support=0.05, min_confidence=0.0)\n",
    "\n",
    "\n",
    "def clean_ids(id_list):\n",
    "    return [int(x) for x in id_list if pd.notnull(x)]\n",
    "\n",
    "#  Top-15 by support \n",
    "general_records = []\n",
    "for rule in sorted(rules, key=lambda r: r.support, reverse=True)[:15]:\n",
    "    ant, cons = set(rule.lhs), set(rule.rhs)\n",
    "    cust_ant  = set(merged_df_clustered[merged_df_clustered['Item_TX'].isin(ant)]['ParentCustomer_CD'])\n",
    "    cust_cons = set(merged_df_clustered[merged_df_clustered['Item_TX'].isin(cons)]['ParentCustomer_CD'])\n",
    "    diff = list(cust_ant - cust_cons)\n",
    "\n",
    "    general_records.append({\n",
    "        'antecedent'      : ', '.join(rule.lhs),\n",
    "        'consequent'      : ', '.join(rule.rhs),\n",
    "        'support'         : rule.support,\n",
    "        'confidence'      : rule.confidence,\n",
    "        'lift'            : rule.lift,\n",
    "        'cust_bought_ant' : len(cust_ant),\n",
    "        'raw_diff_list'   : diff,\n",
    "        'clean_diff_list' : clean_ids(diff),\n",
    "        'diff_count'      : len(clean_ids(diff))\n",
    "    })\n",
    "\n",
    "general_top15 = pd.DataFrame(general_records)\n",
    "\n",
    "#  Next-15 by support \n",
    "general_records_2 = []\n",
    "for rule in sorted(rules, key=lambda r: r.support, reverse=True)[15:30]:\n",
    "    ant, cons = set(rule.lhs), set(rule.rhs)\n",
    "    cust_ant  = set(merged_df_clustered[merged_df_clustered['Item_TX'].isin(ant)]['ParentCustomer_CD'])\n",
    "    cust_cons = set(merged_df_clustered[merged_df_clustered['Item_TX'].isin(cons)]['ParentCustomer_CD'])\n",
    "    diff = list(cust_ant - cust_cons)\n",
    "\n",
    "    general_records_2.append({\n",
    "        'antecedent'      : ', '.join(rule.lhs),\n",
    "        'consequent'      : ', '.join(rule.rhs),\n",
    "        'support'         : rule.support,\n",
    "        'confidence'      : rule.confidence,\n",
    "        'lift'            : rule.lift,\n",
    "        'cust_bought_ant' : len(cust_ant),\n",
    "        'raw_diff_list'   : diff,\n",
    "        'clean_diff_list' : clean_ids(diff),\n",
    "        'diff_count'      : len(clean_ids(diff))\n",
    "    })\n",
    "\n",
    "general_next15 = pd.DataFrame(general_records_2)\n",
    "\n",
    "# Build ParentCustomer_CD - ParentCustomer_TX \n",
    "cust_map = (\n",
    "    merged_df_clustered[['ParentCustomer_CD','ParentCustomer_TX']]\n",
    "    .drop_duplicates()\n",
    "    .set_index('ParentCustomer_CD')['ParentCustomer_TX']\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# IDs  -names \n",
    "for df in (general_top15, general_next15):\n",
    "    df['Missing_ParentCustomer_TXs'] = df['clean_diff_list'].apply(\n",
    "        lambda ids: [cust_map.get(i, f\"ID:{i}\") for i in ids]\n",
    "    )\n",
    "    df['Missing_ParentCustomer_TXs_str'] = df['Missing_ParentCustomer_TXs'].apply(lambda L: \"; \".join(L))\n",
    "\n",
    "# Export\n",
    "out_path = \"G:/Mi unidad/Antwerp/Data engineering/Solved/Market basket/general_mba_support_top30_cleaned_with_names.xlsx\"\n",
    "with pd.ExcelWriter(out_path, engine='openpyxl') as writer:\n",
    "    general_top15 .to_excel(writer, sheet_name='General_Top15', index=False)\n",
    "    general_next15.to_excel(writer, sheet_name='General_Next15', index=False)\n",
    "print(\"Exported with customer-names to:\", out_path)\n",
    "\n",
    "for df, title in [(general_top15, \"Top 15\"), (general_next15, \"Next 15\")]:\n",
    "    pdf = df[df['diff_count'] > 0]\n",
    "    plt.figure(figsize=(8, max(4, len(pdf)*0.5)), tight_layout=True)\n",
    "    sns.barplot(\n",
    "        data=pdf,\n",
    "        y='antecedent',\n",
    "        x='diff_count',\n",
    "        hue='consequent',\n",
    "        dodge=False\n",
    "    )\n",
    "    plt.title(f\" Overall: # Customers Bought Antecedent but NOT Consequent\\n({title} Rules by Support)\")\n",
    "    plt.xlabel(\"Number of Customers Missing Consequent\")\n",
    "    plt.ylabel(\"Antecedent Item\")\n",
    "    plt.legend(title=\"Consequent\", bbox_to_anchor=(1.05,1), loc='upper left')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "659396fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (323616000.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mgit add \"Product_portfolio _patterns.ipynb\"\u001b[39m\n        ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "git add \"Product_portfolio _patterns.ipynb\"\n",
    "git commit -m \"Agrega notebook de patrones de portafolio\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
